{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%env XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "from math import *\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import numpyro as npr\n",
    "import numpyro.distributions as dist\n",
    "import tqdm as tqdm\n",
    "\n",
    "# npr.set_platform('gpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### True model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "L = 10\n",
    "\n",
    "def model_true(y=None, theta=None, rng_key=random.PRNGKey(1)):\n",
    "    key, *subkeys = random.split(rng_key, 5)  # subkeys\n",
    "    \n",
    "    if theta is None:\n",
    "        # Sample from priors \\pi(\\mu, \\sigma^2)\n",
    "        mu = npr.sample('mu', dist.Normal(0, 1), rng_key=subkeys[0])\n",
    "        sigma_sq = npr.sample('sigma_sq', dist.Gamma(1, 1), rng_key=subkeys[1])\n",
    "    else:\n",
    "        mu, sigma_sq = theta\n",
    "    \n",
    "    # The true likelihood, sum of LogNormal rvs.\n",
    "    with npr.plate('L', L):\n",
    "        x = npr.sample('X', dist.LogNormal(mu, sigma_sq), rng_key=subkeys[2])\n",
    "        \n",
    "    data = npr.sample('Y', dist.Normal(x.sum(0), 1e-6), rng_key=subkeys[3], obs=y)\n",
    "    \n",
    "    if theta is None:\n",
    "        return (mu, sigma_sq)\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "    \n",
    "# Test\n",
    "model_true()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Approximate model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_abc(y=None, theta=None, rng_key=random.PRNGKey(1)):\n",
    "    key, *subkeys = random.split(rng_key, 4)  # subkeys\n",
    "    \n",
    "    if theta is None:\n",
    "        # Sample from priors \\pi(\\mu, \\sigma^2)\n",
    "        mu = npr.sample('mu', dist.Normal(0, 1), rng_key=subkeys[0])\n",
    "        sigma_sq = npr.sample('sigma_sq', dist.Gamma(1, 1), rng_key=subkeys[1])\n",
    "    else:\n",
    "        mu, sigma_sq = theta\n",
    "    \n",
    "    # Approximate likelihood\n",
    "    beta_sq = jnp.log((jnp.exp(sigma_sq)-1)/L + 1)\n",
    "    alpha = mu + jnp.log(L) + 0.5*(sigma_sq - beta_sq)\n",
    "        \n",
    "    data = npr.sample('Y', dist.LogNormal(alpha, beta_sq), rng_key=subkeys[2], obs=y)\n",
    "    \n",
    "    if theta is None:\n",
    "        return (mu, sigma_sq)\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "    \n",
    "# Test\n",
    "model_abc()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpyro.infer import SVI, Trace_ELBO, MCMC, NUTS\n",
    "from numpyro.infer.autoguide import *\n",
    "\n",
    "\n",
    "def mcmc(rng_key, model, y, n_samples=20, n_warmup=20, pbar=False):\n",
    "    nuts = MCMC(NUTS(model), num_samples=n_samples, num_warmup=n_warmup, progress_bar=False)\n",
    "\n",
    "    rng_key, rng_subkey = random.split(rng_key)\n",
    "    nuts.run(rng_subkey, y)\n",
    "\n",
    "    samples = nuts.get_samples()\n",
    "    mu = samples['mu'][-1]\n",
    "    sigma_sq = samples['sigma_sq'][-1]\n",
    "    \n",
    "    return mu, sigma_sq\n",
    "\n",
    "\n",
    "def vb_diag(rng_key, model, y, pbar=False):   \n",
    "    key, *subkeys = random.split(rng_key, 4)\n",
    "    \n",
    "    guide = AutoDiagonalNormal(model)\n",
    "    lr = 1e-3\n",
    "    n_iter = 5000\n",
    "\n",
    "    optimizer = npr.optim.ClippedAdam(step_size=lr)\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO(num_particles=100))\n",
    "    svi_result = svi.run(subkeys[0], n_iter, y=y, progress_bar=pbar)\n",
    "    \n",
    "    mu = guide.sample_posterior(subkeys[1], svi_result.params)['mu']\n",
    "    sigma_sq = guide.sample_posterior(subkeys[2], svi_result.params)['sigma_sq']\n",
    "    \n",
    "    return mu, sigma_sq\n",
    "\n",
    "\n",
    "def vb_full(rng_key, model, y, pbar=False):    \n",
    "    key, *subkeys = random.split(rng_key, 4)\n",
    "    \n",
    "    guide = AutoMultivariateNormal(model)\n",
    "    lr = 5e-4\n",
    "    n_iter = 5000\n",
    "\n",
    "    optimizer = npr.optim.ClippedAdam(step_size=lr)\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO(num_particles=100))\n",
    "    svi_result = svi.run(rng_key, n_iter, y=y, progress_bar=pbar)\n",
    "    \n",
    "    mu = guide.sample_posterior(subkeys[1], svi_result.params)['mu']\n",
    "    sigma_sq = guide.sample_posterior(subkeys[2], svi_result.params)['sigma_sq']\n",
    "    \n",
    "    return mu, sigma_sq\n",
    "\n",
    "\n",
    "def laplace(rng_key, model, y, pbar=False):    \n",
    "    key, *subkeys = random.split(rng_key, 4)\n",
    "    \n",
    "    guide = AutoLaplaceApproximation(model)\n",
    "    lr = 1e-3\n",
    "    n_iter = 5000\n",
    "\n",
    "    optimizer = npr.optim.ClippedAdam(step_size=lr)\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO(num_particles=1))\n",
    "    svi_result = svi.run(rng_key, n_iter, y=y, progress_bar=pbar)\n",
    "    \n",
    "    mu = guide.sample_posterior(subkeys[1], svi_result.params)['mu']\n",
    "    sigma_sq = guide.sample_posterior(subkeys[2], svi_result.params)['sigma_sq']\n",
    "    \n",
    "    return mu, sigma_sq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gibbs-prior sampler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "POSTERIORS = ['mcmc', 'vb_diag', 'vb_full', 'laplace', 'mcmc']\n",
    "POSTERIOR_FUNCS = {\n",
    "    'mcmc': mcmc, \n",
    "    'vb_diag': vb_diag, \n",
    "    'vb_full': vb_full, \n",
    "    'laplace': laplace,\n",
    "}\n",
    "\n",
    "\n",
    "def sample_gibbs_prior(rng_key, posterior, T=100):\n",
    "    assert posterior in POSTERIORS\n",
    "    \n",
    "    theta_samples = []\n",
    "    \n",
    "    rng_key, rng_subkey = random.split(rng_key)\n",
    "    y_t  = model_true(y=None, theta=None, rng_key=rng_subkey)\n",
    "    \n",
    "    for t in tqdm.trange(T):     \n",
    "        rng_key, *subkeys = random.split(rng_key, 3)\n",
    "        \n",
    "        # Get q(theta | y_t)\n",
    "        theta_t = POSTERIOR_FUNCS[posterior](subkeys[0], model_abc, y=y_t)\n",
    "        theta_samples.append(np.array(theta_t).copy())\n",
    "        \n",
    "        # Sample y_t, always using the true model\n",
    "        y_t  = model_true(y=None, theta=theta_t, rng_key=subkeys[1])\n",
    "        \n",
    "    return np.array(theta_samples)\n",
    "\n",
    "# Prior\n",
    "def sample_prior(L, n_samples, rng_key=random.PRNGKey(1)):\n",
    "    rng_keys = random.split(rng_key, 2)\n",
    "    mu = npr.sample('mu', dist.Normal(0, 1), sample_shape=(n_samples, 1), rng_key=rng_keys[0])\n",
    "    sigma_sq = npr.sample('sigma_sq', dist.Gamma(1, 1), sample_shape=(n_samples, 1), rng_key=rng_keys[1])\n",
    "    theta = np.concatenate((mu, sigma_sq), axis=-1)\n",
    "    return theta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample $\\pi_G$ for the ABC model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(9999)\n",
    "\n",
    "if not os.path.exists('results/abc'):\n",
    "    os.makedirs('results/abc')\n",
    "    \n",
    "    \n",
    "# Laplace\n",
    "thetas_laplace = sample_gibbs_prior(rng_key, 'laplace', T=10000)\n",
    "np.save(f'../res/abc/laplace_approx', thetas_laplace)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}